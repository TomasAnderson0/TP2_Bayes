---
title: "Informe_Met-Has"
format: pdf
editor: visual
lang: "es"
---
# Introducción

La estadística bayesiana se usa para poder dar respuestas a incertidumbres sobre parámetros que afectan a distintas situaciones y para ello, se utilizan dos fuentes de información. La primera es lo que uno cree conocer sobre los parámetros previamente, es decir, la información a *priori*. La segunda es que tan compatibles son los valores de los parametros con los datos observados, denominada como la *verosimilitud*. Juntando estas informaciones se obtiene lo que se conoce como la distribución a *posteriori* del o los parámetros. Pero para poder encontrar respuesta al desconocimiento sobre el tema se debe poder sacar muestras de la distribución a *posteriori*. Para realizar esto, una de las técnicas que se pueden utilizar es Metropolis-Hastings.

# Metropolis-Hastings

Esta tecnica es un método de Montecarlo basado en cadenas de Markov que consiste, a grandes rasgos, en recorrer los distintos valores de una distribución de probabilidad generando un secuencia de S valores posibles $x$ de dicha distribución $x(1),x(2),…,x(S)$ donde para obtener $x(i+1)$ usamos $x(i)$. De esta manera se quiere que el conjunto de los valores simulados se aproximen lo máximo posible a una muestra real de la distribución.

Esta técnica funciona de la siguiente manera:

1.  En la iteración i estamos en el valor $x(i)$
2.  En función del valor actual $x(i)=x$, proponemos un nuevo valor $x′$ en función de $q(x′∣x)$ siendo $q$ la distribución de salto que uno debe proponer, que sera la que presente los puntos de salto posibles.
3.  Decidimos si vamos a la nueva ubicación $x(i+1)=x′$ o si nos quedamos en $x(i+1)=x$:
    i.  Calcular la probabilidad de salto:
        -   $α_{x→x′} = min(1,f(x′)/f(x))$
    ii. Saltar a $x′$ con probabilidad $α_{x→x′}$:

```{=tex}
\begin{center}
$x^{(i+1)} =
\begin{cases}
x' \text{ con probabilidad } α_{x→x′}\\
x \text{  con probabilidad } α_{x→x′}
\end{cases}$

\end{center}
```
La función para utilizar el método es la siguiente:

```{r, eval=FALSE}
# n: Tamaño de la cadena que se quiere generar

# p_inicial: Primer punto con el que se inicial la cadena.
# Es igual a 0 por defecto.

# v_random: Utilizar si se quiere empezar la cadena con un punto aleatorio.
# Introducir un vector de dos números para obtener un número aleatorio
# entre ellos dos.

# d_objetivo: Función que calcula la densidad de la función que se
# quiere muestrear.

# r_propuesta: Función que genera un punto de la distribución de salto con la
# media sin especificar. Por defecto, se utiliza una normal estándar.

# d_propuesta: Función que devuelve la densidad en un punto de la distribución
# de salto con la media sin especificar. Por defecto, se utiliza una normal
# estándar.

sample_mh_1d = function(n, p_inicial = 0, d_objetivo, r_propuesta = rnorm,
                        d_propuesta = dnorm, v_random = NA){
  stopifnot(n > 0)
  muestras = numeric(n)
  #Seleccion del primer valor de la cadena
  if (length(v_random) == 2){
    muestras[1] = runif(1,min(v_random),max(v_random))
  } else {
    muestras[1] = p_inicial
  }
  #Generación de todos los n-1 puntos siguientes de la cadena
  for (i in 2:n) {
    #Proposición del nuevo valor
    puntos = valor_salto_1d(muestras, r_propuesta, i)
    p_actual = puntos[1]
    p_nuevo = puntos[2]
    #Cálculo de la probabilidad de salto
    prob = salto_1d(p_actual, p_nuevo, d_objetivo, d_propuesta)
    #Saltamos?
    salto = test_salto_1d(prob)
    #Salto
    muestras[i] = p_gen_1d(salto, p_actual, p_nuevo)
  }
    return(muestras)
}

#Donde las funciones utilizadas son:
#Proposición del nuevo valor
valor_salto_1d = function(muestras, r_propuesta, i){
  p_actual = muestras[i-1]
  p_nuevo = r_propuesta(p_actual)
  puntos = c(p_actual, p_nuevo)
  return(puntos)
}

#Cálculo de la probabilidad de salto
salto_1d = function(p_actual, p_nuevo, d_objetivo, d_propuesta) {
  f_nuevo = d_objetivo(p_nuevo)
  f_actual = d_objetivo(p_actual)
  q_actual = d_propuesta(p_actual, p_nuevo)
  q_nuevo = d_propuesta(p_nuevo, p_actual)
  prob = (f_nuevo*q_actual) / (q_nuevo*f_actual)
  alfa = min(1, prob)
  return(alfa)
}

#Saltamos?
test_salto_1d = function(alfa){
  result = rbinom(1,1,alfa)
  return(result)
}

#Salto
p_gen_1d = function(result, p_actual, p_nuevo) {
  if (result){
    return(p_nuevo)
  } else {
    return(p_actual)
  }
}
```

# Aplicación de Metropolis-Hastings en una dimensión

```{r, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE}
source("Funciones.R")
library(ggplot2)
library(mvtnorm)
library(pracma)
tema = theme(panel.grid = element_line(color = "lightgrey"),
             panel.background = element_rect(fill = "#f8f8f8"),
             axis.line = element_line(color = "black"))
```

## Distribución de Kumaraswamy

La distribución de Kumaraswamy es una distribución de probabilidad continua que se utiliza para modelar variables aleatorias con soporte en el intervalo $[0,1]$. Su función de densidad se parece a la beta pero su expresión matemática resulta ser de cómputo más sencillo. Dicha función es:


\begin{center}
$\begin{array}{lr}
p(x \mid a, b) = a b x ^ {a - 1} (1 - x ^ a)^{b - 1} & \text{con } a, b > 0
\end{array}$
\end{center}

Para observar que forma tiene, se grafica su función de densidad para 5 valores de a y b.

\begin{table}[h!]
\begin{center}
\begin{tabular}{ |c| c c c c c| }
\hline
\multicolumn{6}{|c|}{Valores} \\
\hline
 & Curva 1 & Curva 2 & Curva 3 & Curva 4 & Curva 5 \\
\hline
  a & 2 & 10 & 5 & 2 & 0.5 \\
\hline
  b & 2 & 2 & 15 & 15 & 0.33 \\
\hline
\end{tabular}
\caption{Parametros de las distribuciones}
\label{table:1}
\end{center}
\end{table}



```{r, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE, fig.align='right', fig.cap = "Curvas de densidad de Kumaraswamy"}
kumaraswamy = function(a,b,x) {
  stopifnot(a > 0)
  stopifnot(b > 0)
  valor = a*b*(x^(a-1))*((1-(x^a))^(b-1))
  return(valor)
}

grilla = seq(0,1,.01)

parametros = matrix(c(2,2,10,2,5,15,2,15,1/2,1/3), nrow = 5, ncol = 2, byrow = T)

Valores = matrix(0,nrow = 101, ncol = 5)
for (i in 1:5) {
  Valores[,i] = kumaraswamy(parametros[i,1],parametros[i,2],grilla)
}

Val = c(Valores[,1], Valores[,2], Valores[,3], Valores[,4], Valores[,5]) 

data_kuma = data.frame(x = rep(grilla, 5), y = Val, color = as.factor(rep(1:5, each = 101)))

ggplot(data_kuma) + geom_line(aes(x = x, y = y, color = color)) + scale_color_discrete(labels = c("Curva 1", "Curva 2", "Curva 3", "Curva 4", "Curva 5"), type = c("red", "yellow3", "blue", "green", "brown"), name = "") + tema +
  scale_y_continuous(name = "Densidad") +
  scale_x_continuous(name = "X")
```

Es una función sencilla donde su forma de densidad resulta muy flexible al variar a y b, por lo que se puede usar fácilmente para la representación de la probabilidad a *priori* de parámetros que esten entre 0 y 1.

Usando la función de Metropolis-Hastings se obtiene un muestra de 5000 observaciones de la distribución de Kumaraswamy con $a = 6$ y $b = 2$.
Se utiliza una distribución beta reparametrizada en su media y su concentración para proponer los nuevos valores y para obtener el primer valor de la cadena se genera aleatoriamente con igual probabilidad un punto entre 0 y 1 .

Se usan tres valores de concentración diferentes para evaluar la eficiencia del método.

```{r, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE, fig.cap = "Cadenas generadas de la distribución de Kumaraswamy según concentración "}
r_beta_by_mean = function(n, media, con){
  alfa = media * con
  beta = con - alfa
  rbeta(n, alfa, beta)
}

d_beta_by_mean = function(x, media, con){
  alfa = media * con
  beta = con - alfa
  dbeta(x, alfa, beta)
}
set.seed(69)

d_objetivo_kuma = function(x) kumaraswamy(6, 2, x)

r_propuesta_kuma_2 = function(x) r_beta_by_mean(1, x, 5)

d_propuesta_kuma_2 = function(x, media) d_beta_by_mean(x, media, 5)

muestras_kuma_2 = sample_mh_1d(5000, v_random = c(0,1), d_objetivo = d_objetivo_kuma, 
                             d_propuesta = d_propuesta_kuma_2, 
                             r_propuesta = r_propuesta_kuma_2)

r_propuesta_kuma_3 = function(x) r_beta_by_mean(1, x, 10)

d_propuesta_kuma_3 = function(x, media) d_beta_by_mean(x, media, 10)

muestras_kuma_3 = sample_mh_1d(5000, v_random = c(0,1), d_objetivo = d_objetivo_kuma, 
                               d_propuesta = d_propuesta_kuma_3, 
                               r_propuesta = r_propuesta_kuma_3)

r_propuesta_kuma_1 = function(x) r_beta_by_mean(1, x, 1)

d_propuesta_kuma_1 = function(x, media) d_beta_by_mean(x, media, 1)

muestras_kuma_1 = sample_mh_1d(5000, v_random = c(0,1), d_objetivo = d_objetivo_kuma, 
                               d_propuesta = d_propuesta_kuma_1, 
                               r_propuesta = r_propuesta_kuma_1)



muestras_kuma = data.frame(obs = c(muestras_kuma_1, muestras_kuma_2, muestras_kuma_3),
                           orden = rep(1:5000,3),
                           cadena = as.factor(rep(c(1,5,10), each = 5000)))

label_facet <- function(original_var, custom_name){
  lev <- levels(as.factor(original_var))
  lab <- paste0(custom_name, ": ", lev)
  names(lab) <- lev
  return(lab)  
}

ggplot(muestras_kuma) + geom_line(aes(x = orden, y = obs)) + facet_grid(vars(cadena)) + scale_x_continuous(name = "Observación") +
  scale_y_continuous(name = "X") + facet_wrap(~ cadena, labeller = labeller(cadena = label_facet(muestras_kuma$cadena, "Concentración")), dir = "v")

tasa_kuma_1 = 1-mean(muestras_kuma_1[1:4999]-muestras_kuma_1[2:5000] == 0)
tasa_kuma_2 = 1-mean(muestras_kuma_2[1:4999]-muestras_kuma_2[2:5000] == 0)
tasa_kuma_3 = 1-mean(muestras_kuma_3[1:4999]-muestras_kuma_3[2:5000] == 0)

```



\begin{table}[h!]
\begin{center}
\begin{tabular}{ |c| c c c| }
\hline
Concentración & 1 & 5 & 10  \\
\hline
  Tasa de aceptación & `r round(tasa_kuma_1,3)-.001` & `r round(tasa_kuma_2,3)` & `r round(tasa_kuma_3,3)` \\
\hline
\end{tabular}
\caption{Evaluación de las cadenas de Kumaraswamy}
\label{tabla:1}
\end{center}
\end{table}



En la figura 2 se observa que el método funciona mejor con una concentración alta porque oscila más veces recorriendo la densidad de una manera más eficiente. Esto se verifica observando las tasas de aceptación de los saltos en la tabla 2.


```{r, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE, fig.width=8, fig.cap = "Distribuciones de las observaciones generadas de la Kumaraswamy"}


ggplot(muestras_kuma) + geom_histogram(aes(x = obs, fill = cadena), color = "#4b4b4b", bins = 24 ) + facet_wrap(~ cadena, labeller = labeller(cadena = label_facet(muestras_kuma$cadena, "Concentración"))) + tema +
  scale_x_continuous("X") +
  scale_y_continuous("Frecuencia") +
  scale_fill_discrete(guide = "none",labels = c("1", "5", "10"), name = c("Concentración"), type = c("slateblue2", "salmon1", "seagreen2"))
```

```{r, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE, fig.cap = ""}
acf_kuma_1 = acf(muestras_kuma_1, lag.max = Inf, plot = F)
acf_kuma_2 = acf(muestras_kuma_2, lag.max = Inf, plot = F)
acf_kuma_3 = acf(muestras_kuma_3, lag.max = Inf, plot = F)

lag_kuma = max(which(acf_kuma_1$acf<.05)[1], which(acf_kuma_2$acf<.05)[1], which(acf_kuma_3$acf<.05)[1])

acf_kuma = data.frame(ACF = c(acf_kuma_1$acf[1:lag_kuma], acf_kuma_2$acf[1:lag_kuma], acf_kuma_3$acf[1:lag_kuma]), Rezago = rep(0:(lag_kuma-1), 3), Concentracion = as.factor(rep(c(1, 5, 10), each = lag_kuma)))



ggplot(acf_kuma) + geom_line(aes(x = Rezago, y = ACF, color = Concentracion)) + geom_point(aes(x = Rezago, y = ACF)) + facet_wrap(~ Concentracion, labeller = labeller(Concentracion = label_facet(acf_kuma$Concentracion, "Concentración"))) + geom_hline(yintercept = 0) + geom_vline(xintercept = 0) +
geom_hline(yintercept =0.05, linetype = "dashed", color = "blue") + geom_hline(yintercept = -0.05, linetype = "dashed", color = "blue") +
  scale_color_discrete(guide = "none",labels = c("1", "5", "10"), name = c("Concentración"), type = c("slateblue2", "salmon1", "seagreen2")) + tema
```

```{r, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}
media_kuma_1 = mean(muestras_kuma_1)
media_kuma_2 =mean(muestras_kuma_2)
media_kuma_3 =mean(muestras_kuma_3)


percentil_5_1_kuma = mean(sort(muestras_kuma_1)[250:251])
percentil_5_2_kuma = mean(sort(muestras_kuma_2)[250:251])
percentil_5_3_kuma = mean(sort(muestras_kuma_3)[250:251])

percentil_95_1_kuma = mean(sort(muestras_kuma_1)[4750:4751])
percentil_95_2_kuma = mean(sort(muestras_kuma_2)[4750:4751])
percentil_95_3_kuma = mean(sort(muestras_kuma_3)[4750:4751])

logit_kuma_1 = log(muestras_kuma_1/(1-muestras_kuma_1))
logit_kuma_2 = log(muestras_kuma_2/(1-muestras_kuma_2))
logit_kuma_3 = log(muestras_kuma_3/(1-muestras_kuma_3))

media_logit1_kuma = mean(logit_kuma_1)
media_logit2_kuma = mean(logit_kuma_2)
media_logit3_kuma = mean(logit_kuma_3)


percentil_5_logit1_kuma = mean(sort(logit_kuma_1)[250:251])
percentil_5_logit2_kuma = mean(sort(logit_kuma_2)[250:251])
percentil_5_logit3_kuma = mean(sort(logit_kuma_3)[250:251])

percentil_95_logit1_kuma = mean(sort(logit_kuma_1)[4750:4751])
percentil_95_logit2_kuma = mean(sort(logit_kuma_2)[4750:4751])
percentil_95_logit3_kuma = mean(sort(logit_kuma_3)[4750:4751])
```



Las tres cadenas convergen a la misma distribución y la cantidad de rezagos que tardan hasta llegar a una autocorrelación despreciable (<0.05) es baja y similar en todas por lo que el método funciona correctamente aún variando la dispersión de la distribución del salto.


\newpage
\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
  & Media & Percentil 5 & Percentil 95 \\ 
\hline
$X_1$  & `r media_kuma_1`  & `r percentil_5_1_kuma` & `r percentil_95_1_kuma` \\
$X_5$  & `r media_kuma_2` & `r percentil_5_2_kuma` & `r percentil_95_2_kuma` \\
$X_{10}$  & `r media_kuma_3` & `r percentil_5_3_kuma` & `r percentil_95_3_kuma`  \\
$logit(X_1)$ & `r media_logit1_kuma` & `r percentil_5_logit1_kuma` & `r percentil_95_logit1_kuma`  \\
$logit(X_5)$ & `r media_logit2_kuma` & `r percentil_5_logit2_kuma` & `r percentil_95_logit2_kuma`   \\
$logit(X_{10})$ & `r media_logit3_kuma` & `r percentil_5_logit3_kuma` & `r percentil_95_logit3_kuma ` \\ \hline

\end{tabular}
\caption{Medidas descriptivas estimadas de las distribuciones}
\label{tabla:1}
\end{center}
\end{table}

Donde:
$${\displaystyle \operatorname {logit} (X)=\log \left({\frac {X}{1-X}}\right)=\log(X)-\log(1-X)\!\,}$$



Todas las medidas descriptivas de X son parecidas para las distintas cadenas generadas, pero si se le aplica la transformación *logit* a X esas diferencias mínimas se agrandan un poco.















# Aplicación de Metropolis-Hastings en dos dimensiones

La verdadera utilidad del método presentado anteriormente se encuentra en la extracción de muestras de distribuciones multivariadias. Igual que la situación anterior, no es necesario que las funciones que se quieren muestrear esten normalizadas. Se presentan dos situaciones en las que se puede aplicar la técnica. La primera, un ejemplo simple donde se obtienen muestras de la distribución normal bivariada donde este método funciona bien y la segunda, una función difícil de obtener muestras válidas en donde se pueden ver las fallas de Metropolis-Hastings.

Para realizar estas muestras, no sirve la función presentada en la primera parte del informe. Por eso se tiene la necesidad de modificar dicha función.




```{r, eval=FALSE}
# Se utiliza como distribución de salto propuesto una normal bivariada

# n: Tamaño de la cadena que se quiere generar

# p_inicial: Primer punto con el que se inicial la cadena. Es igual a 0 por
# defecto.

# d_objetivo: Función que calcule la densidad de la función que se quiere
# muestrear.

# matrix_var: Matrix de varianzas y covarianzas de la función de salto normal
# bivariada.

sample_mh_2d = function(n, p_inicial , d_objetivo, matrix_var = diag(1,2)){
  stopifnot(n > 0)
  muestras = matrix(nrow = n,ncol = 2)
#Selección del primer valor de la cadena
  muestras[1,] = p_inicial

  #Generación de todos los n-1 puntos siguientes de la cadena
  for (i in 2:n) {
    #Proposición del nuevo valor
    puntos = valor_salto_2d(muestras, matrix_var, i)
    p_actual = puntos[1:2]
    p_nuevo = puntos[3:4]
    
    #Cálculo de la probabilidad de salto
    prob = salto_2d(p_actual, p_nuevo, d_objetivo, matrix_var)
    
    #Saltamos?
    salto = test_salto_2d(prob)
    
    #Salto
    muestras[i,] = p_gen_2d(salto, p_actual, p_nuevo)
  }
  if (length(p_inicial) == 1) {
    return(as.vector(muestras))
  }
  return(muestras)
}

#Proposición del nuevo valor

valor_salto_2d = function(muestras, matrix_var, i){
  p_actual = muestras[i-1,]
  p_nuevo = rmvnorm(1, mean = p_actual, sigma = matrix_var)
  puntos = c(p_actual, p_nuevo)
  return(puntos)
}

#Cálculo de la probabilidad de salto

salto_2d = function(p_actual, p_nuevo, d_objetivo, matrix_var) {
  f_nuevo = d_objetivo(p_nuevo)
  f_actual = d_objetivo(p_actual)
  q_actual = dmvnorm(p_actual, p_nuevo, sigma = matrix_var)
  q_nuevo = dmvnorm(p_nuevo, p_actual, sigma = matrix_var)
  prob = (f_nuevo*q_actual) / (q_nuevo*f_actual)
  alfa = min(1, prob)
  return(alfa)
}

#Saltamos?

test_salto_2d = function(alfa){
  result = rbinom(1,1,alfa)
  return(result)
  }

#Salto

p_gen_2d = function(result, p_actual, p_nuevo) {
  if (result){
     return(p_nuevo)
  } else {
    return(p_actual)
  }
}
```




## Distribución normal bivariada

La distribución a muestrear en este caso será una normal bivariada.

$$
p(\boldsymbol{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) =
    \frac{1}{(2\pi)^{k/2} |\boldsymbol{\Sigma}|^{1/2}}
    \exp\left(-\frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\boldsymbol{x} - \boldsymbol{\mu})\right)
$$

Donde $\mu$ es el vector de esperanzas y $\Sigma$ la matrix de varianzas y covarianzas.

Se controla que tan buenas son las muestras obtenidas a través de la evaluación de la cadena generada por el método y de qué tanto se asimilan las probabilidades estimadas por la distribución muestral obtenida a las reales. La normal bivariada de interés viene dada por los siguientes parámetros.


$$
\begin{array}{lr}
    \boldsymbol{\mu}^* = \begin{bmatrix} 0.4 \\ 0.75 \end{bmatrix}
    & \boldsymbol{\Sigma}^* = \begin{bmatrix} 1.35 & 0.4 \\ 0.4 & 2.4 \\ \end{bmatrix}
\end{array}
$$


```{r, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}
mu <- c(0.4, 0.75)
sigma <- matrix(c(1.35, 0.4, 0.4, 2.4), nrow = 2)
x <- seq(-4, 4.5, length.out = 100)
y <- seq(-3, 5, length.out = 100)
grid <- expand.grid(x = x, y = y)

# Calcular la densidad
grid$density <- dmvnorm(grid, mean = mu, sigma = sigma)

ggplot(grid, aes(x = x, y = y, z = density)) +
  geom_contour_filled() +
  labs(title = "",
       x = "X1",
       y = "X2",
       z = "Densidad") +
  theme_minimal() +
  theme(legend.position = "none") +
  coord_fixed(ratio = 1)

```






Usando la función de Metropolis-Hastings se obtiene un muestra de 5000 observaciones. Para obtener las muestras, primero se debe seleccionar la matriz de covarianzas de la distribución de salto. Esto se logra evaluando el número de muestras efectivas de las dos dimensiones de las muestras generadas por separado, variando los valores en la matriz y quedandose con la que tenga el mayor número de muestras efectivas totales.

El cálculo realizado es: 

$$N_{eff} = \frac{S}{1 + 2 \sum_{k=1}^N ACF(k)}$$
Donde $S$ es el número de unidades generadas en la cadena, $ACF(k)$ la autocorrelación de la cadena con $k$ rezagos y N la cantidad de rezagos hasta que $ACF(k)<0.05$ por primera vez.


```{r, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}
#poner cálculo
var = matrix(0,nrow = 85, ncol = 3)
var[85,] = c(1.85, 2.15, 0.5)
```

Los valores de la matriz de varianza evaluados son:

$$
\begin{array}{lrrrr}
\boldsymbol{\Sigma}_{Salto} = \begin{bmatrix} \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2 \end{bmatrix} & \sigma_1^2\in[0.85,1.85] ;&
\sigma_2^2\in[1.90,2.90]; &
\sigma_{12}\in[0.20,0.60] \\
\end{array}
$$

La combinación de dichos valores con el mayor número de muestras efectivas totales es `r var[85,1]`, `r var[85,2]` y `r var[85,3]` para $\sigma_1^2$, $\sigma_2^2$ y $\sigma_{12}$ respectivamente.



```{r, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE, fig.cap = "Cadenas generadas con distintas matrices de covarianzas"}
set.seed(69)

d_objetivo_normal = function (x) dmvnorm(x, mean = c(.4, .75), sigma = matrix(c(1.35, .4, .4, 2.4), nrow = 2))


muestra_normal = sample_mh_2d(5000, p_inicial = c(1,1), d_objetivo = d_objetivo_normal, matrix_var = matrix(c(var[85,1], var[85,3], var[85,3], var[85,2]), nrow = 2))

muestra_normal_df = data.frame(obs = c(muestra_normal[,1],muestra_normal[,2]),
                           orden = rep(1:5000,2),
                           cadena = rep(c("x1", "x2"), each = 5000))

ggplot(muestra_normal_df) + geom_line(aes(x = orden, y = obs)) + facet_grid(vars(cadena))+ scale_x_continuous(name = "Observación") +
  scale_y_continuous(name = "X")



nef_norm = apply(muestra_normal, 2, FUN = neef)
```
```{r, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE, fig.cap = "Autocorrelación de las cadenas con distinta matriz de covarianzas"}
acf_norm1 = acf(muestra_normal[,1], lag.max = Inf, plot = F)
acf_norm2 = acf(muestra_normal[,2], lag.max = Inf, plot = F)

lag_norm = max(which(acf_norm1$acf<.05)[1],which(acf_norm2$acf<.05)[1])

acf_norm = data.frame(ACF = c(acf_norm1$acf[1:lag_norm], acf_norm2$acf[1:lag_norm]), Rezago = rep(0:(lag_norm-1),2), Dim = rep(c("X1", "X2"), each = lag_norm))



ggplot(acf_norm) + geom_line(aes(x = Rezago, y = ACF, color = Dim)) +
facet_wrap(~Dim)  +
geom_point(aes(x = Rezago, y = ACF)) + geom_hline(yintercept = 0) + geom_vline(xintercept = 0) +
geom_hline(yintercept =0.05, linetype = "dashed", color = "blue") + geom_hline(yintercept = -0.05, linetype = "dashed", color = "blue") +
  scale_color_discrete(guide = "none", labels = c("1", "5", "10"), type = c("slateblue2", "salmon1")) + 
  tema
```
Las cadenas en la figura x se ven relativamenta aleatorias y parece recorrer el soporte de la función objetivo, que es indicio de una autocorrelación baja entre las observación generadas. Esto se aprecia en la figura x+1, donde la autocorrelación de ambas cadenas disminuye rápidamente al aumentar el rezago.
El número de muestras efectivas para $x_1$ y $x_2$ es `r nef_norm[1]` y `r nef_norm[2]` respectivamente. Estos son valores relativamente grandes, que dan una imagen positiva de la calidad de la muestra generada.


```{r, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}
Prob_mh_norm_1 = mean(muestra_normal[, 1] > 1 & muestra_normal[, 2] < 0)
Prob_mh_norm_2 = mean(muestra_normal[, 1] > 1 & muestra_normal[, 2] > 2)
Prob_mh_norm_3 = mean(muestra_normal[, 1] > .4 & muestra_normal[, 2] > .75)


rmv = rmvnorm(100000, mean = c(.4, .75), sigma = matrix(c(1.35, .4, .4, 2.4), nrow = 2))
Prob_rmv_1 =pmvnorm(lower = c(1, -Inf), upper = c(Inf, 0), mean = c(0.4, 0.75), sigma = matrix(c(1.35, 0.4, 0.4, 2.4), nrow = 2))[1]
Prob_rmv_2 =pmvnorm(lower = c(1, 2), upper = c(Inf, Inf), mean = c(0.4, 0.75), sigma = matrix(c(1.35, 0.4, 0.4, 2.4), nrow = 2))[1]
Prob_rmv_3 =pmvnorm(lower = c(0.4, 0.75), upper = c(Inf, Inf), mean = c(0.4, 0.75), sigma = matrix(c(1.35, 0.4, 0.4, 2.4), nrow = 2))[1]

Error1 = round(abs(Prob_mh_norm_1 - Prob_rmv_1)/Prob_rmv_1, 4)
Error2 = round(abs(Prob_mh_norm_2 - Prob_rmv_2)/Prob_rmv_2, 4)
Error3 = round(abs(Prob_mh_norm_3 - Prob_rmv_3)/Prob_rmv_3, 4)
```
Se calculan tres probabilidades usando las muestras generadas y se comparan con las reales para ver que tan precisas son.


\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Probabilidades  & $\hat{\mathbf{X}}$ & $\mathbf{X}$ & Error relativo \\
\hline
$\mathrm{Pr}(X_1 > 1, X_2 < 0)$ &  `r Prob_mh_norm_1`  & `r round(Prob_rmv_1, 4)`   & `r Error1`  \\
\hline
$\mathrm{Pr}(X_1 > 1, X_2 > 2)$ &  `r Prob_mh_norm_2`  &  `r round(Prob_rmv_2, 4)`  & `r Error2`  \\
\hline
$\mathrm{Pr}(X_1 > 0.40, X_2 > 0.75)$ & `r Prob_mh_norm_3`  & `r round(Prob_rmv_3, 4)`  &  `r Error3` \\
\hline
\end{tabular}
\caption{Evaluación de las probabilidades estimadas con las muestras generadas por M-H}
\label{table:1}
\end{center}
\end{table}


En la tabla 4 se observa que cuando se quiere estimar la probabilidad de un evento correspondiente a un área lejana de la masa central de probabilidades, la precisión de la aproximación disminuye, es decir, aumenta el error. Aún así, el método logra obtener una muestra aceptable al no ser tan grande el error relativo en dichos casos.



## Función de Rosenbrock

El "valle de Rosenbrock", "banana de Rosenbrock" o simplemente función de Rosenbrock es una función matemática utilizada comunmente en problemas de optimización y prueba para algoritmos de optimización numérica.

La función está definida por:
$$
f(x, y) = (a - x) ^ 2 + b(y - x^2) ^ 2
$$
En estadística bayesiana, en ciertos casos, se obtiene una densidad de distribución a *posteriori* que toma una forma semejante a dicha función. 


Un ejemplo de esto es la función $p^*$:

$$
p^*(x_1, x_2 \mid a, b) = \exp \left\{-\left[(a - x_1) ^ 2 + b(x_2 - x_1^2) ^ 2\right] \right\}
$$

```{r, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE, fig.height = 4, fig.width = 7, fig.cap = "Distribución de la p* con a = 0.5 y b = 0.5"}
x <- seq(-1.5, 2.1, length.out = 100)
y <- seq(-1, 4.25, length.out = 100)
grid <- expand.grid(x = x, y = y)

d_objetivo_rosenbrock = function (x) exp((-1)*(((.5 - x[1])^2) + 5 * ((x[2] - (x[1]^2))^2)))

grid$density <- d_objetivo_rosenbrock(grid)

ggplot(grid, aes(x = x, y = y, z = density$x)) +
  geom_contour_filled() +
  labs(title = "", fill = "",
       x = "X1",
       y = "X2",
       z = "Densidad") +
  theme(legend.position = "none") + theme_minimal() 

```




Se quiere probar que tan bien puede Metropolis-Hastings generar muestras de dicha función de probabilidad y para ello se proponen distintas matrices de varianzas y covarianzas de la distribución de salto propuesta evaluando el rendimiento obtenido con cada una de ellas. Se utilizan los valores $0.5$ y $5$ para los parámetros a y b respectivamente para la generación de las 1000 observaciones.



\begin{table}[h!]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
       & Muestra 1 & Muestra 2 & Muestra 3 \\ \hline
$\sigma_1^2$ & 1         & 1         & 1 \\ \hline
$\sigma_2^2$ & 1         & 1         & 2   \\ \hline
$\sigma_{12}$ & 0.7       & 0.3       & 0  \\ \hline
\end{tabular}
\caption{Valores de las matrices de varianzas de cada muestra generada.}
\label{table:1}
\end{center}
\end{table}


```{r, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE, fig.cap = "Cadenas generadas con distintas matrices de covarianzas"}
rosenbrock = function (x) ((.5 - x(1))^2) + 5 * ((x(2) - (x(1)^2))^2)

d_objetivo_rosenbrock = function (x) exp((-1)*(((.5 - x[1])^2) + 5 * ((x[2] - (x[1]^2))^2)))

set.seed(69)

muestra_rosenbrock_1 = sample_mh_2d(1000, p_inicial = c(1,1), d_objetivo = d_objetivo_rosenbrock,
                              matrix_var = matrix(c(1, .7, .7, 1), nrow = 2))
muestra_rosenbrock_2 = sample_mh_2d(1000, p_inicial = c(1,1), d_objetivo = d_objetivo_rosenbrock,
                                    matrix_var = matrix(c(1, .3, .3, 1), nrow = 2))
muestra_rosenbrock_3 = sample_mh_2d(1000, p_inicial = c(1,1), d_objetivo = d_objetivo_rosenbrock,
                                    matrix_var = matrix(c(1,0,0,2), nrow = 2))


muestra_rosenbrock_df_1 = data.frame(obs = c(muestra_rosenbrock_1[,1],muestra_rosenbrock_1[,2]),
                               orden = rep(1:1000,2),
                               cadena = rep(c("X1", "X2"), each = 1000),
                               sigma = rep("Muestra 1",1000))


muestra_rosenbrock_df_2 = data.frame(obs = c(muestra_rosenbrock_2[,1],muestra_rosenbrock_2[,2]),
                                     orden = rep(1:1000,2),
                                     cadena = rep(c("X1", "X2"), each = 1000),
                                     sigma = rep("Muestra 2",1000))

muestra_rosenbrock_df_3 = data.frame(obs = c(muestra_rosenbrock_3[,1],muestra_rosenbrock_3[,2]),
                                     orden = rep(1:1000,2),
                                     cadena = rep(c("X1", "X2"), each = 1000),
                                     sigma = rep("Muestra 3",1000))

muestra_rosenbrock_df = rbind(muestra_rosenbrock_df_1, muestra_rosenbrock_df_2) 


ggplot(muestra_rosenbrock_df) + geom_line(aes(x = orden, y = obs)) + 
  facet_wrap(~cadena + sigma, ncol = 2) + scale_x_continuous(name = "Observación") + scale_y_continuous(name = "X")



```




```{r, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}
nef_rosenbrock_1 = apply(muestra_rosenbrock_1, 2, FUN = neef)
nef_rosenbrock_2 = apply(muestra_rosenbrock_2, 2, FUN = neef)
nef_rosenbrock_3 = apply(muestra_rosenbrock_3, 2, FUN = neef)
```


En comparación con la figura x del caso Normal bivariado, se puede notar que las cadenas de la figura x tienen una tasa de aceptación menor, generando más estancamientos en un valor y menos picos. Además, el número efectivo de muestras es menor.





```{r, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE, fig.cap = "Autocorrelación según la muestra"}
tasa_rosen_1 = 1-mean(muestra_rosenbrock_1[1:999,]-muestra_rosenbrock_1[2:1000,] == 0)
tasa_rosen_2 = 1-mean(muestra_rosenbrock_2[1:999,]-muestra_rosenbrock_2[2:1000,] == 0)
tasa_rosen_3 = 1-mean(muestra_rosenbrock_3[1:999,]-muestra_rosenbrock_3[2:1000,] == 0)

acf_rosen_x1_1 = acf(muestra_rosenbrock_1[,1], lag.max = Inf, plot = F)
acf_rosen_x2_1 = acf(muestra_rosenbrock_1[,2], lag.max = Inf, plot = F)
acf_rosen_x1_2 = acf(muestra_rosenbrock_2[,1], lag.max = Inf, plot = F)
acf_rosen_x2_2 = acf(muestra_rosenbrock_2[,2], lag.max = Inf, plot = F)
acf_rosen_x1_3 = acf(muestra_rosenbrock_3[,1], lag.max = Inf, plot = F)
acf_rosen_x2_3 = acf(muestra_rosenbrock_3[,2], lag.max = Inf, plot = F)


lag_rosen_1 = max(which(acf_rosen_x1_1$acf<.05)[1], which(acf_rosen_x2_1$acf<.05)[1])
lag_rosen_2 = max(which(acf_rosen_x1_2$acf<.05)[1], which(acf_rosen_x2_2$acf<.05)[1])
lag_rosen_3 = max(which(acf_rosen_x1_3$acf<.05)[1], which(acf_rosen_x2_3$acf<.05)[1])
lag_rosen = max(lag_rosen_1, lag_rosen_2, lag_rosen_3)

acf_rosen_1 = data.frame(ACF = c(acf_rosen_x1_1$acf[1:lag_rosen], acf_rosen_x2_1$acf[1:lag_rosen]), Rezago = rep(0:(lag_rosen-1),2), Dim = rep(c("X1", "X2"), each = lag_rosen), Cov = "Muestra 1")

acf_rosen_2 = data.frame(ACF = c(acf_rosen_x1_2$acf[1:lag_rosen], acf_rosen_x2_2$acf[1:lag_rosen]), Rezago = rep(0:(lag_rosen-1),2), Dim = rep(c("X1", "X2"), each = lag_rosen), Cov = "Muestra 2")

acf_rosen_3 = data.frame(ACF = c(acf_rosen_x1_3$acf[1:lag_rosen], acf_rosen_x2_3$acf[1:lag_rosen]), Rezago = rep(0:(lag_rosen-1),2), Dim = rep(c("X1", "X2"), each = lag_rosen), Cov = "Muestra 3")

acf_rosen = rbind(acf_rosen_1, acf_rosen_2, acf_rosen_3)


ggplot(acf_rosen) + geom_line(aes(x = Rezago, y = ACF, color = Cov)) +
facet_wrap(~Dim*Cov)  +
geom_point(aes(x = Rezago, y = ACF), size = .1) + geom_hline(yintercept = 0) + geom_vline(xintercept = 0) +
geom_hline(yintercept =0.05, linetype = "dashed", color = "blue") + geom_hline(yintercept = -0.05, linetype = "dashed", color = "blue") +
  scale_color_discrete(guide = "none", labels = c("1", "5", "10"), type = c("slateblue2", "salmon1","seagreen2")) +
  tema



```


En la figura x, todas las cadenas bajan lentamente hasta tener una autocorrelación despreciable, pero la segunda lo hace de forma más rápida.  



\begin{table}[h!]
\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
   & Muestra 1 & Muestra 2 & Muestra 3 \\ \hline
X1 & `r nef_rosenbrock_1[1]`& `r nef_rosenbrock_2[1]`& `r nef_rosenbrock_3[1]`
\\ \hline
X2 & `r nef_rosenbrock_1[2]`& `r nef_rosenbrock_2[2]` & `r nef_rosenbrock_3[2]`
\\ \hline
\end{tabular}
\caption{Número efectivo de muestras}
\label{table:1}
\end{center}
\end{table}


El número de muestras efectivas para cada cadena es bajo. Esto tiene sentido, al ver los resultados de la figura x y x.
```{r, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}
Prob_mh_rosen_1 = mean(muestra_rosenbrock_1[, 1] < 1 & muestra_rosenbrock_1[, 1] > 0 & muestra_rosenbrock_1[, 2] > 0 & muestra_rosenbrock_1[, 2] < 1)
Prob_mh_rosen_2 = mean(muestra_rosenbrock_1[, 1] < 0 & muestra_rosenbrock_1[, 1] > -1 & muestra_rosenbrock_1[, 2] > 0 & muestra_rosenbrock_1[, 2] < 1)
Prob_mh_rosen_3 = mean(muestra_rosenbrock_1[, 1] < 2.2 & muestra_rosenbrock_1[, 1] > 1 & muestra_rosenbrock_1[, 2] > 2 & muestra_rosenbrock_1[, 2] < 3)


ro = function (x, y) exp((-1)*(((.5 - x)^2) + 5 * ((y - (x^2))^2)))
int = integral2(ro, xmin = -261, xmax =  261, ymin = -261, ymax = 261)$Q[1]


Prob_int_1 =integral2(ro, xmin = 0, xmax =  1, ymin = 0, ymax = 1)$Q[1]/int
Prob_int_2 =integral2(ro, xmin = -1, xmax =  1, ymin = 0, ymax = 1)$Q[1]/int
Prob_int_3 =integral2(ro, xmin = 1, xmax =  2, ymin = 2, ymax = 3)$Q[1]/int


error_1_rosen =abs(Prob_mh_rosen_1 - Prob_int_1)/Prob_int_1
error_2_rosen =abs(Prob_mh_rosen_2 - Prob_int_2)/Prob_int_2
error_3_rosen =abs(Prob_mh_rosen_3 - Prob_int_3)/Prob_int_3
```


Se calculan tres probabilidades usando las muestras generadas y se comparan con las reales para ver que tan precisas son.


\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Probabilidades  & $\hat{\mathbf{X}}$ & $\mathbf{X}$ & Error relativo \\
\hline
$\mathrm{Pr}(0 < X_1 < 1, 0 < X_2 < 1)$ &  `r Prob_mh_rosen_1`  & `r round(Prob_int_1, 3)`   &  `r error_1_rosen` \\
\hline
$\mathrm{Pr}(-1 < X_1 < 0, 0 < X_2 < 1)$ &  `r Prob_mh_rosen_2`  &  `r round(Prob_int_2, 3)`  & `r error_2_rosen`  \\
\hline
$\mathrm{Pr}(1 < X_1 < 2, 2 < X_2 < 3)$ & `r Prob_mh_rosen_3`  & `r round(Prob_int_3, 3)`  &  `r error_3_rosen` \\
\hline
\end{tabular}
\caption{Evaluación de las probabilidades estimadas con las muestras generadas por M-H}
\label{table:1}
\end{center}
\end{table}

Los errores relativos de la tabla 7 son muy grande, por lo que da un indicio de lo mala que es la estimación de la probabilidad. Por esto, y todo lo comentado anteriormente, es razonable pensar que la distribución de las muestras generadas por Metropolis-Hastings difieren en gran medida de la verdadera distribución¨de $p*$.


# Conclusiones 


Metropolis-Hastings es un método útil en ciertas ocasiones. Es simple, facil de programar y entender, y además se sabe que cuando el número de muestras tiende a infinito, la distribución de probabilidad de la secuencia tiende a la verdadera función objetivo, pero en casos donde la función de densidad de la distribución sea compleja es poco preciso. Para obtener una muestra con buena representatividad, es decir, que haya explorado el rango completo de la distribución, se requerira una muestra de tamaño grande. Entonces, el método no sera de utilidad al requerir tantas iteraciones para llegar a un resultado aceptable. Además, al aumentar la dimensión se incrementa el tiempo que le toma a la computadora realizar los cálculos. Esto, combinado con el hecho de que hay métodos más eficientes hace a Metropolis-Hastings un método no recomendado. 
Para obtener muestras de una función de densidad, es preferible el uso de otros métodos como Hamiltonian-Montecarlo.






